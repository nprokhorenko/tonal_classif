{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"keras_tonal_classif.ipynb","version":"0.3.2","provenance":[{"file_id":"1Ql9lK53SFYhrkbnKwsBIUjSilnXBKHDd","timestamp":1565630005146}],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"n-aAlY6owBxG","colab_type":"code","colab":{}},"source":["\n","\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import auc\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import keras\n","from keras import models\n","from keras import layers\n","from keras.utils import to_categorical\n","from keras.datasets import imdb\n","np.load.__defaults__= (None, True, True, 'ASCII')\n","import sys\n","\n","\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve\n","\n","\n","\n","from sklearn.metrics import confusion_matrix \n","from sklearn.metrics import accuracy_score \n","from sklearn.metrics import classification_report \n","\n","\n","\n","\n","np.set_printoptions(threshold=sys.maxsize)\n","%matplotlib inline\n","\n","\n","\n","word_index = imdb.get_word_index()\n","\n","\n","word_index = {k:(v+3) for k,v in word_index.items()}\n","word_index[\"<PAD>\"] = 0\n","word_index[\"<START>\"] = 1\n","word_index[\"<UNK>\"] = 2  \n","word_index[\"<UNUSED>\"] = 3\n","\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","def decode_review(text):\n","  return ' '.join([reverse_word_index.get(i, '?') for i in text])\n","  \n"," \n","\n","\n","(training_X, training_y), (testing_X, testing_y) = imdb.load_data(num_words=10000)\n","X_text = np.concatenate((training_X, testing_X), axis = 0)\n","data = np.concatenate((training_X, testing_X), axis=0)\n","targets = np.concatenate((training_y, testing_y), axis=0)\n","\n","\n","def vectorize(sequences, dimension = 10000):\n","  res = np.zeros((len(sequences), dimension))\n","  for i, sequence in enumerate(sequences):\n","    res[i, sequence] = 1\n","  return res\n"," \n","data = vectorize(data)\n","targets = np.array(targets).astype(\"float32\")\n","\n","\n","\n","\n","\n","X_test = data[:10000]\n","y_test = targets[:10000]\n","X_train = data[10000:]\n","y_train = targets[10000:]\n","model = models.Sequential()\n","\n","\n","\n","model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000,)))\n","model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n","model.add(layers.Dense(50, activation = \"relu\"))\n","model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n","model.add(layers.Dense(50, activation = \"relu\"))\n","model.add(layers.Dense(1, activation = \"sigmoid\"))\n","model.summary()\n","\n","\n","\n","#keras.optimizers.Adam(lr=0.1)\n","#keras.activations.relu(x, alpha=0.0, max_value=None, )\n","def plot_roc_curve(fpr,tpr): \n","  plt.plot(fpr,tpr) \n","  plt.axis([0,1,0,1]) \n","  plt.xlabel('False Positive Rate') \n","  plt.ylabel('True Positive Rate') \n","  plt.show()    \n","  \n","#plot_roc_curve (fpr,tpr) \n","model.compile(\n"," optimizer = \"adam\",\n"," loss = \"binary_crossentropy\",\n"," metrics = [\"accuracy\"]\n",")\n","\n","\n","\n","\n","# Обучение первоначальной модели\n","history = model.fit(\n"," X_train, y_train,\n"," epochs= 2,\n"," batch_size = 400,\n"," validation_data = (X_test, y_test)\n",")\n","\n","\n","\n","#Визуализация матрицы невязки по двум классам \n","predicted = model.predict(data, verbose=1)\n","#predicted = np.transpose(predicted)\n","predict = np.array([])\n","predict = [0] * len(data)\n","error_matrix  = np.array([])\n","error_matrix = [0] * len(data)\n","\n","for x in np.arange(len(predicted)):\n","   if predicted[x] >= 0.5:\n","      predict[x] = 1.\n","   elif predicted[x] < 0.5:\n","      predict[x] = 0.\n","predict = np.transpose(predict)\n","\n","for x in np.arange(len(targets)):\n","  if(targets[x] == predict[x]):\n","    error_matrix[x] = 1.\n","  else:\n","    error_matrix[x] = 0.\n","error_matrix = np.transpose(error_matrix)\n","print(\"Матрица невязки по двум классам:\")    \n","print(error_matrix)    \n","\n","\n","# Построение ROC кривой\n","y_pred_keras = model.predict(X_test).ravel()\n","fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\n","\n","auc_keras = auc(fpr_keras, tpr_keras)\n","\n","rf = RandomForestClassifier(max_depth=3, n_estimators=10)\n","rf.fit(X_train, y_train)\n","\n","y_pred_rf = rf.predict_proba(X_test)[:, 1]\n","fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)\n","auc_rf = auc(fpr_rf, tpr_rf)\n","\n","plt.figure(1)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n","plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.title('ROC curve')\n","plt.legend(loc='best')\n","plt.show()\n","\n","plt.figure(2)\n","plt.xlim(0, 0.2)\n","plt.ylim(0.8, 1)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n","plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.title('ROC curve (zoomed in at top left)')\n","plt.legend(loc='best')\n","plt.show()\n","\n","\n","#Вывод ревью, на которых лучшая модель ошиблась\n","print(\"Вывод ревью, на которых лучшая модель ошиблась:\")\n","for i in np.arange(10000):\n","  if error_matrix[i] == 0.:\n","      print(decode_review(X_text[i]))\n","\n","\n","print(\"Test-Accuracy:\", np.mean(history.history[\"val_acc\"]))\n","#print(\"Test-Accuracy:\", np.mean(results.history[keras_metrics.precision(), keras_metrics.recall()]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cj5ivPqWyyaX","colab_type":"text"},"source":["\n","Судя по отзывам на которых ошиблась модель, ошибка могла произойти из большого числа\n","неизвестных слов и малого количества слов прилагательных, которые описывают фильм.\n"]},{"cell_type":"markdown","metadata":{"id":"M43WhoJCytvl","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"VHPQ-0-Iwibn","colab_type":"text"},"source":["С кодом всё понятно, но не мог бы ты визуализировать результаты обучения?\n","\n","1) Оформи все в Jupyter Notebook.\n","\n","2) В ноутбуке сохрани записи о точности модели на валидационной выборке в зависимости от количества пройденных эпох, параметра learning rate в оптимизаторе, трешхолда (по умолчанию трешхолд = 0.5, то есть если предсказание от 0 до 0.5 - ревью отрицательное, если от 0.5 до 1 - положительное).\n","\n","3) Визуализируй матрицу невязки по 2 классам (положительное/негативное ревью), посчитай метрики precision/recall для отрицательных ревью, построй график ROC-кривой.\n","\n","4) Выведи ревью, на которых лучшая модель ошиблась, сделай предположение, почему могла произойти ошибка.\n","\n","5) Попробуй усложнить архитектуру, используй LSTM слои (примеры реализации можно найти в интернете даже по этой же задаче), сравни точность с начальной моделью.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w0D4j_JzwgXk","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"5Rp9wlg9wjUc","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"-fLsJ95wQ356","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESyfkMTxFDok","colab_type":"code","colab":{}},"source":["# Модель для подсчитывания precision и recall для отрицательных отзывов\n","import numpy as np\n","import keras_metrics\n","from keras.utils import to_categorical\n","from keras import models\n","from keras import layers\n","from keras.datasets import imdb\n","\n","np.load.__defaults__=(None, True, True, 'ASCII')\n","\n","(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n","data = np.concatenate((training_data, testing_data), axis=0)\n","targets = np.concatenate((training_targets, testing_targets), axis=0)\n","def vectorize(sequences, dimension = 10000):\n"," results = np.zeros((len(sequences), dimension))\n"," for i, sequence in enumerate(sequences):\n","  results[i, sequence] = 1\n"," return results\n","\n","data = vectorize(data)\n","targets = np.array(targets).astype(\"float32\")\n","\n","\n","\n","\n","i = 0\n","# Создаем матрицы отрицательных ревью\n","\n","\n","\n","X_negative = np.zeros((50000, 10000))\n","X_negative = np.array(X_negative).astype(\"float32\")\n","\n","\n","# Цикл очень долго работает, а другого пути отобрать негативные отзывы я не нашел\n","for i in np.arange(50000):\n","  if targets[i] == 0.:\n","     #np.append(X_negative, data[i])\n","     np.append(X_negative, data[i])\n","\n","\n","     \n","y_negative = np.zeros(25000)\n","\n","print(data[0:2])\n","\n","\n","print(\"************************\")\n","\n","print(X_negative[0:2])\n","sys.exit()\n","\n","\n","model = models.Sequential()\n","\n","model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n","model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n","model.add(layers.Dense(50, activation = \"relu\"))\n","model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n","model.add(layers.Dense(50, activation = \"relu\"))\n","model.add(layers.Dense(1, activation = \"sigmoid\"))\n","\n","model.summary()\n","\n","model.compile(\n"," optimizer = \"adam\",\n"," loss = \"binary_crossentropy\",\n"," metrics = [\"accuracy\"]\n"," #metrics=[keras_metrics.precision(), keras_metrics.recall()]\n",")\n","\n","history = model.fit(\n"," X_negative, y_negative,\n"," validation_split=0.20,\n"," epochs = 2,\n"," batch_size = 400 \n",")\n","\n","print(history.history)\n","print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INV4JQMlpweh","colab_type":"text"},"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"WXbezJiRyS4a","colab_type":"text"},"source":["\n","\n"]},{"cell_type":"code","metadata":{"id":"-UOlJsgMmSuM","colab_type":"code","colab":{}},"source":["\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LaGWIhgin6ji","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"5PCtFbHqn3Ci","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"PsFuON-zlyc3","colab_type":"code","colab":{}},"source":["# Модель с использованием LSTM слоев\n","\n","from __future__ import print_function\n","\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding\n","from keras.layers import LSTM\n","from keras.datasets import imdb\n","\n","max_features = 10000\n","# cut texts after this number of words (among top max_features most common words)\n","maxlen = 80\n","batch_size = 32\n","\n","print('Loading data...')\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(x_train), 'train sequences')\n","print(len(x_test), 'test sequences')\n","\n","print('Pad sequences (samples x time)')\n","x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)\n","\n","print('Build model...')\n","model = Sequential()\n","model.add(Embedding(max_features, 128))\n","model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# try using different optimizers and different optimizer configs\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","print('Train...')\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=15,\n","          validation_data=(x_test, y_test))\n","score, acc = model.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Test score:', score)\n","print('Test accuracy:', acc)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"27eJRugSMh2j","colab_type":"code","colab":{}},"source":["0.8937499892711639 -  0.8078\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voJ3hxYyMjBd","colab_type":"text"},"source":["### При использовании слоев LSTM точность сети падает. Скорее всего для лучшего качества на LSTM необходимо больше данных. \n","\n","*25000/25000 [==============================] - 27s 1ms/step\n","Test score: 1.1325414541220664*\n","Test accuracy: **0.8078**\n","\n","При использовании обычных слоев точность модели: **0.8937499892711639**\n","\n","**Разница** =  0.8937499892711639 -  0.8078 = 0.08594998927116393"]},{"cell_type":"markdown","metadata":{"id":"VAEnDC4MMiQW","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"QhLL2N4oWF0w","colab_type":"text"},"source":["\n","Точность так же падает при увеличении количества эпох на первой модели.\n","\n","Точность не меняется при learning rate = 0.1, 0.01, 0.001.\n"]},{"cell_type":"markdown","metadata":{"id":"fi5T6wyPVu-f","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"au1Rh9C1KZo7","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"qJ9oSVN_VC8B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2Glp5LwxVV1X"},"source":["\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TlMyRJciVVOc"},"source":["\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZqHcr88eVURc"},"source":["\n","\n"]},{"cell_type":"code","metadata":{"id":"tCGp-oB3Kbkq","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LEY2q2fwK1Eq","colab_type":"text"},"source":[""]}]}